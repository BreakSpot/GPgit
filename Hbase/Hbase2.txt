1、client端发出get命令
2、到zookeeper中找的meta表所在的regionServer位置
3、从regionServer的meta表中找到信息所在的region
4、getvalue，先访问memstore，如果没有再访问Hfile

Client写入->存入memstore，一直到满->flush成一个storefile，直到增长到一定阈值->触发Compact合并->多个storefile合并成一个storefile，同时进行版本合并和数据删除->当storefile Compact之后，逐步形成越来越大的storeFile->当个storeFile打小超过一定阈值后触发Spailt，把当前的Region Split成2个Region，Region会下线，新Split出的2个子Region会被HMaster分配到相应的HRegion上，使得原先的1个Region的压力分流到2个Region上。
注意：这里的数据删除是真正的数据删除，之前的del只是打上一个标签。

组件：
Client:
    1).整个Hbase集群的访问入口
    2).适应HbaseRPC机制与HMaster和HRegionServer进行通信
    3).与HMaster进行通信进行管理类操作
    4).与HRegionServer进行数据读写操作
    5).包含访问HBase的接口，并维护cache来加快对HBase的访问
Zookeeper：
    1).保证任何时候，集群中只有一个HMaster
    2).存储所有HRegion的寻址入口
    3).监控上下线，并通知Master
    4).存储HBase的schema和table的数据
    5). 存储meta表的位置
HMaster:
    1).HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master在运行主要负责Table和Region的管理工作
    2).管理用户对table的增删改查操作
    3).管理HRegionServer的负载均衡，调整Region分布
    4).Region Split后，负责新Region的分布
    5).在HRgionServer停机后，负责失效HRegionServer上Region迁徙工作
HRegion Server
    1).维护HRegion，处理对这些HRegion的IO请求，向HDFS文件系统中读写数据
    2).负责切分在运行过程中变得过大的HRegion
    3).Client访问HBase上的数据的过程并不需要master参与（寻址访问ZooKeeper和HRegionServer，数据读写访问HRegionServer），HMaster仅仅维护table和Region的原数据负载很低
HBase和Zookeeper的关系：
    1).HBase依赖Zookeeper
    2).默认情况下，HBase管理Zookeeper实例，启动或暂停Zookeeper
    3).HMaster与HRegionServer启动时会Zookeeper注册
    4).Zookeeper的引入使得HMaster不再是单点故障


数据迁移：
    put API
    bulk load tool：CSV变成Hfile文件 快，压力小
    MapReduce job
    




